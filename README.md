# LKtransformers - å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€
[2025-08-10 prefillæå‡ï¼Œ32èŠ‚ç‚¹ä¸ºæœ€ä¼˜é€Ÿåº¦ï¼ŒåŒæ—¶ä¼˜åŒ–äº†è¶…çº¿ç¨‹æ”¯æŒ]
[2025-08-02 ä¿®å¤sh install.shåœ¨shellä¸ºdashæ—¶çš„å…¼å®¹posixé—®é¢˜ï¼Œå¯¼è‡´æœªèƒ½å®‰è£…è‡ªå¸¦flashinfer,å‡ºç°çš„ä¸€äº›è«åé—®é¢˜
RuntimeError: pidfd_getfd: Operation not permittedï¼Œä½¿ç”¨PYTORCH_CUDA_ALLOC_CONF=expandable_segments:TrueåæŠ¥é”™ç­‰
æ›´æ–°ä»£ç é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh æˆ–è€… USE_BALANCE_SERVE=1 USE_NUMA=1 sh install.sh]

[2025-08-01 è§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜]

[2025-07-25 å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€]

æœ¬åˆ†æ”¯æä¾› NUMA ä¼˜åŒ–çš„ç¨³å®šç‰ˆæœ¬ï¼ŒåŒ…å«å†…å­˜/æ˜¾å­˜ä¼˜åŒ–å’Œä¸€äº›é—®é¢˜ä¿®å¤ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **NUMA å†…å­˜ä¼˜åŒ–**ï¼šå¤šä¸ª NUMA èŠ‚ç‚¹å…±äº«å•ä»½å†…å­˜ï¼Œ32èŠ‚ç‚¹è´Ÿè½½å‡è¡¡ï¼Œè§£ç é€Ÿåº¦ä¸é™ç•¥æœ‰æå‡
- **çº¿ç¨‹ç²¾ç»†æ§åˆ¶**ï¼šé€šè¿‡ `THREADS_PER_NODE` ç¯å¢ƒå˜é‡ç®¡ç†æ¯ä¸ªèŠ‚ç‚¹çš„è®¡ç®—çº¿ç¨‹æ•°
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šæ–°å¢ `VLinearMarlin16` æ”¯æŒï¼Œè§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜
- **å·²éªŒè¯æ¨¡å‹**ï¼š
  - `KVCache-ai/Kimi-K2-Instruct-GGUF`
  - `deepseek-ai/DeepSeek-R1-0528`
  - `unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF`

## âš ï¸ å·²çŸ¥é—®é¢˜

1. æ”¯æŒ AMX çš„ CPU ä½¿ç”¨ amx é…ç½®æ–‡ä»¶ä¼šæŠ¥é”™ï¼ˆAMX åå°NUMAæ”¹é€ æœªå®Œæˆï¼‰
2. Prefill æ€§èƒ½ä¸‹é™ï¼ˆå·²è§£å†³ï¼‰
3. æŠ¥é”™ `RuntimeError: pidfd_getfd: Operation not permitted`å¯ä»¥å»æ‰PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True è¿è¡Œï¼Œé€Ÿåº¦å˜æ…¢ï¼Œæäº¤é—®é¢˜åˆ°è¿™é‡Œ(å·²è§£å†³ï¼Œ é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh æˆ–è€… USE_BALANCE_SERVE=1 USE_NUMA=1 sh install.sh)
4. Intel è‡³å¼ºå¹³å°ï¼Œæˆ–è€…å¼€å¯è¶…çº¿ç¨‹è¿è¡Œé€Ÿåº¦æ…¢ï¼ˆå·²è§£å†³ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…æ­¥éª¤

git clone https://github.com/guqiong96/ktransformers.git

git checkout full-support-numa

git submodule update --init --recursive --verbose

USE_BALANCE_SERVE=1 USE_NUMA=1 sh install.sh

### è¿è¡Œç¤ºä¾‹
THREADS_PER_NODE=8 python ~/Downloads/KTransformers/ktransformers/server/main.py \
    --gguf_path ~/Models/Kimi-K2-Instruct-GGUF  \
    --model_path ~/Models/Kimi-K2-Instruct \
    --model_name Kimi-K2  \
    --cpu_infer 28 \
    --max_new_tokens 16384 \
    --cache_lens 16384 \
    --cache_q4 true \
    --temperature 0.6 \
    --top_p 0.95 \
    --optimize_config_path ~/Downloads/KTransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml \
    --force_think \
    --use_cuda_graph \
    --host 0.0.0.0 \
    --port 8070 \
    --max_batch_size 4 \
    --backend_type balance_serve

## ğŸ”§ é…ç½®æŠ€å·§

- **NUMA çº¿ç¨‹é…ç½®**ï¼š
  - 8èŠ‚ç‚¹ Ã— 8çº¿ç¨‹ = 64è®¡ç®—çº¿ç¨‹
  - 32èŠ‚ç‚¹ Ã— 2çº¿ç¨‹ = 64è®¡ç®—çº¿ç¨‹
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šåœ¨ YAML é…ç½®ä¸­ä½¿ç”¨ `VLinearMarlin16` é˜²æ­¢ 16G æ˜¾å¡æ˜¾å­˜æº¢å‡º

## ğŸ“Œ æ³¨æ„äº‹é¡¹

1. æ›´æ–°åå‡ºç°ç–‘éš¾é—®é¢˜ï¼Œè¿è¡Œ USE_BALANCE_SERVE=1 USE_NUMA=1 sh install.sh
2. æ›´å¤šå®‰è£…é—®é¢˜è¯·å‚è€ƒä¸»çº¿æ–‡æ¡£
3. å®šæœŸåˆå¹¶ä¸»çº¿è·å–æœ€æ–°ç‰¹æ€§

![Weixin Image_2025-08-02_183944_421](https://github.com/user-attachments/assets/8f227407-2e0c-48b1-a677-ff0545b5b7a8)
 
