# LKtransformers - å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€ï¼AMXæ”¯æŒGGUFæ¨¡å‹ï¼Œå†…å­˜å ç”¨ä¸ç¿»å€, æ”¯æŒNUMAåŠ é€Ÿï¼

kimi k2 ä¹‹åçš„æ¨¡å‹æ”¯æŒåœæ­¢ï¼Œéœ€è¦è¿è¡Œæ–°æ¨¡å‹ï¼Œè¯·ç§»æ­¥åˆ°Lvllmé¡¹ç›®ï¼šhttps://github.com/guqiong96/Lvllm
 
[2025-09-05 AMXé¢„è§ˆç‰ˆï¼šæ”¯æŒGGUFæ¨¡å‹ï¼Œå†…å­˜å ç”¨ä¸ç¿»å€ï¼Œæ”¯æŒNUMAåŠ é€Ÿï¼Œé¢„è§ˆç‰ˆq4é‡åŒ–æµ‹è¯•é€šè¿‡ï¼Œ ä¼˜åŒ–äº†ç¼–è¯‘å‚æ•°]
![e8163807efefcda003b87ab567bd0488](https://github.com/user-attachments/assets/4ea39111-de19-487b-bb5c-91c456f3f211)


[2025-08-29 å°†è®¡ç®—è¿›ç¨‹æ”¾ä¸»è¿›ç¨‹è¿è¡Œï¼Œæ¥å£ã€è°ƒåº¦æ”¾å­è¿›ç¨‹ï¼Œæ–¹ä¾¿è°ƒè¯•åŠåŠæ—¶å‘ç°é”™è¯¯æç¤º] 

[2025-08-22 prefillä½¿ç”¨ä¸€æ¬¡å¤štonkençŸ©é˜µè®¡ç®—çš„numaæœ€ä¼˜åŒ–ç‰ˆæœ¬ï¼Œ å¢åŠ ç¯å¢ƒå˜é‡LK_POWER_SAVINGï¼Œé»˜è®¤ä¸ºå…³é—­ï¼Œå¼€å¯æ—¶ä½¿ç”¨ç¯å¢ƒå˜é‡LK_POWER_SAVING=1 å¯ä»¥ç¼“è§£å†…å­˜è¿‡çƒ­é™é€Ÿ]

[2025-08-21 é™ä½çº¿ç¨‹å ç”¨CPUï¼Œé¿å…å†…å­˜æ¸©åº¦ä¸Šå‡è¿‡å¿«ï¼Œprefillåˆ‡æ¢è‡³numaæœ€ä¼˜åŒ–ç‰ˆæœ¬]

[2025-08-18 prefillæé€Ÿ10%, ä½¿ç”¨ç¯å¢ƒå˜é‡LK_THREADSè°ƒèŠ‚æ€§èƒ½]

[2025-08-14 AVX2 æ¿€æ´»å‡½æ•°]

[2025-08-12 decodeæå‡é€Ÿåº¦ï¼Œ32èŠ‚ç‚¹ä¸ºæœ€ä¼˜é€Ÿåº¦]

[2025-08-10 prefillæå‡é€Ÿåº¦ï¼Œ32èŠ‚ç‚¹ä¸ºæœ€ä¼˜é€Ÿåº¦ï¼ŒåŒæ—¶ä¼˜åŒ–äº†è¶…çº¿ç¨‹æ”¯æŒ]

[2025-08-02 ä¿®å¤sh install.shåœ¨shellä¸ºdashæ—¶çš„å…¼å®¹posixé—®é¢˜ï¼Œå¯¼è‡´æœªèƒ½å®‰è£…è‡ªå¸¦flashinfer,å‡ºç°çš„ä¸€äº›è«åé—®é¢˜
RuntimeError: pidfd_getfd: Operation not permittedï¼Œä½¿ç”¨PYTORCH_CUDA_ALLOC_CONF=expandable_segments:TrueåæŠ¥é”™ç­‰
æ›´æ–°ä»£ç é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh]

[2025-08-01 è§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜]

[2025-07-25 å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€]

æœ¬åˆ†æ”¯æä¾› NUMA ä¼˜åŒ–çš„ç¨³å®šç‰ˆæœ¬ï¼ŒåŒ…å«å†…å­˜/æ˜¾å­˜ä¼˜åŒ–å’Œä¸€äº›é—®é¢˜ä¿®å¤ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **NUMA å†…å­˜ä¼˜åŒ–**ï¼šå¤šä¸ª NUMA èŠ‚ç‚¹å…±äº«å•ä»½å†…å­˜ï¼Œ32èŠ‚ç‚¹è´Ÿè½½å‡è¡¡ï¼Œè§£ç é€Ÿåº¦ä¸é™ç•¥æœ‰æå‡
- **çº¿ç¨‹ç²¾ç»†æ§åˆ¶**ï¼šé€šè¿‡ `LK_THREADS` ç¯å¢ƒå˜é‡ç®¡ç†è®¡ç®—çº¿ç¨‹æ•°
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šæ–°å¢ `VLinearMarlin16` æ”¯æŒï¼Œè§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜
- **å·²éªŒè¯æ¨¡å‹**ï¼š
  - `KVCache-ai/Kimi-K2-Instruct-GGUF`
  - `deepseek-ai/DeepSeek-R1-0528`
  - `unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF Qwen3-Coder-30B-A3B-Instruct-UD-Q8_K_XL.gguf `
    <img width="1000" height="1364" alt="image" src="https://github.com/user-attachments/assets/0eaef315-71a4-4371-9cac-a54e3e640f80" />

  - `unsloth/DeepSeek-V3.1-GGUF/UD-Q4_K_XL`
    <img width="1000" height="1364" alt="image" src="https://github.com/user-attachments/assets/493eb4f6-b807-4408-841b-da0d8046e94d" />


## âš ï¸ å·²çŸ¥é—®é¢˜

1. æ”¯æŒ AMX çš„ CPU ä½¿ç”¨ amx é…ç½®æ–‡ä»¶ä¼šæŠ¥é”™ï¼ˆå·²è§£å†³ï¼šAMXå¼€å¯æ— éœ€ç‰¹åˆ«é…ç½®ï¼‰
2. Prefill æ€§èƒ½ä¸‹é™ï¼ˆå·²è§£å†³ï¼‰
3. æŠ¥é”™ `RuntimeError: pidfd_getfd: Operation not permitted`å¯ä»¥å»æ‰PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True(å·²è§£å†³ï¼Œ é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh)
4. Intel è‡³å¼ºå¹³å°ï¼Œæˆ–è€…å¼€å¯è¶…çº¿ç¨‹è¿è¡Œé€Ÿåº¦æ…¢ï¼ˆå·²è§£å†³ï¼‰
5. æ¨ç†æµ‹è¯•å¤šè½®ä»¥åæˆ–ä½¿ç”¨é•¿è¾“å…¥ä¼šè¯å‡ºç°é”™è¯¯ï¼Œæ£€æŸ¥å¹¶ç¡®ä¿cache_lens = max_new_tokens + 2048ï¼Œ å¹¶ä¸”cache_lensã€max_new_tokensä¸º 1024 çš„å€æ•°ï¼Œ å¦‚è¦è®¾ç½®max_batch_sizeï¼Œ æ˜¾å­˜è¶³å¤Ÿçš„æƒ…å†µä¸‹å¢åŠ ç›¸åº”å€æ•°çš„cache_lensã€‚
6. Eingine error: The size of tensor a (4)must match the size of tensor b (5) at non-singleton dimension 0 ,

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…æ­¥éª¤

1. å®‰è£…CUDA 12.8

```bash
# å¸è½½æ—§ç‰ˆæœ¬CUDAå’ŒNVIDIAé©±åŠ¨
sudo /usr/local/cuda/bin/cuda-uninstaller
sudo nvidia-uninstall

# ä¸‹è½½å¹¶å®‰è£…CUDA 12.8 
wget https://developer.download.nvidia.com/compute/cuda/12.8.1/local_installers/cuda_12.8.1_570.124.06_linux.run
sudo sh cuda_12.8.1_570.124.06_linux.run

# ç–‘éš¾é—®é¢˜
å¦‚ä½•å½»åº•å¸è½½ç³»ç»Ÿè‡ªå¸¦cuda ï¼šhttps://github.com/guqiong96/Lvllm/issues/5 
```

2. åˆ›å»ºå¹¶æ¿€æ´»Pythonç¯å¢ƒåŠä¸€äº›ç³»ç»Ÿåº“

```bash
conda create -n lktransformers python==3.12.11
conda activate lktransformers

# å‡çº§libstdcxx-ng  
conda install -c conda-forge libstdcxx-ng

# å®‰è£…NUMAåº“ ubuntu
sudo apt-get install libnuma-dev
# å®‰è£…NUMAåº“ rocky linux
sudo dnf install numactl-devel
```

3. å®‰è£…PyTorch 2.8.0 
pip uninstall triton torchvision torch
pip install triton torchvision torch==2.8.0
 

4. å…‹éš†ä»“åº“å¹¶å®‰è£…ä¾èµ–

git clone --recursive https://github.com/guqiong96/lktransformers.git

cd lktransformers

git submodule update --init --recursive --force

USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh

### å¦‚æœå·²æœ‰æºç å®‰è£…æˆåŠŸè¿‡çš„æ›´æ–°å‡çº§æ­¥éª¤

cd lktransformers

git pull

USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh

æºç ç¬¬ä¸‰æ–¹åº“ä¸å…¨çš„æƒ…å†µè¿è¡Œ:

cd lktransformers

git submodule update --init --recursive --force

### è¿è¡Œç¤ºä¾‹
LK_THREADS=62 python ~/Downloads/lktransformers/ktransformers/server/main.py \
    --gguf_path ~/Models/Kimi-K2-Instruct-GGUF  \
    --model_path ~/Models/Kimi-K2-Instruct-GGUF \
    --model_name Kimi-K2-Instruct-GGUF  \
    --cpu_infer 28 \
    --max_new_tokens 8192 \
    --cache_lens 16384 \
    --cache_q4 true \
    --temperature 0.6 \
    --top_p 0.95 \
    --optimize_config_path ~/Downloads/lktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml \
    --force_think \
    --use_cuda_graph \
    --host 0.0.0.0 \
    --port 8070 \
    --backend_type balance_serve \
    --chunk_size 1024 \
    --memory_gpu_only true



## ğŸ”§ ä¼˜åŒ–è®¾ç½®

- **NUMA çº¿ç¨‹é…ç½®**ï¼š
  - å¼€å¯æœ€å¤šNUMAèŠ‚ç‚¹ï¼ŒAMD EPYC å¼€å¯ L3 As NUMA Domain 

- **æ˜¾å­˜ä¼˜åŒ–**ï¼šåœ¨ YAML é…ç½®ä¸­ä½¿ç”¨ `VLinearMarlin16` é˜²æ­¢ 16G æ˜¾å¡æ˜¾å­˜æº¢å‡º

- **memory kv cache **ï¼š éƒ¨åˆ†æ¨¡å‹å…è®¸ä½¿ç”¨å‚æ•° memory_gpu_only: false, å…è®¸ä½¿ç”¨å†…å­˜å­˜å‚¨æ›´å¤§çš„kv cache


## ğŸ“Œ æ³¨æ„äº‹é¡¹

1. æ›´æ–°åå‡ºç°ç–‘éš¾é—®é¢˜
  cd lktransformers

  git pull

  USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh

  æºç ç¬¬ä¸‰æ–¹åº“ä¸å…¨çš„æƒ…å†µè¿è¡Œ:

  cd lktransformers

  git pull

  git submodule update --init --recursive --force

  USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh
  
2. æ›´å¤šå®‰è£…é—®é¢˜è¯·å‚è€ƒä¸»çº¿æ–‡æ¡£
 






 
