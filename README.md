# LKtransformers - å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€

[2025-08-21 é™ä½çº¿ç¨‹å ç”¨CPUï¼Œé¿å…å†…å­˜æ¸©åº¦ä¸Šå‡è¿‡å¿«ï¼Œprefillåˆ‡æ¢è‡³numaæœ€ä¼˜åŒ–ç‰ˆæœ¬]

[2025-08-18 prefillæé€Ÿ10%, ä½¿ç”¨ç¯å¢ƒå˜é‡LK_THREADSè°ƒèŠ‚æ€§èƒ½]

[2025-08-14 AVX2 æ¿€æ´»å‡½æ•°]

[2025-08-12 decodeæå‡é€Ÿåº¦ï¼Œ32èŠ‚ç‚¹ä¸ºæœ€ä¼˜é€Ÿåº¦]

[2025-08-10 prefillæå‡é€Ÿåº¦ï¼Œ32èŠ‚ç‚¹ä¸ºæœ€ä¼˜é€Ÿåº¦ï¼ŒåŒæ—¶ä¼˜åŒ–äº†è¶…çº¿ç¨‹æ”¯æŒ]

[2025-08-02 ä¿®å¤sh install.shåœ¨shellä¸ºdashæ—¶çš„å…¼å®¹posixé—®é¢˜ï¼Œå¯¼è‡´æœªèƒ½å®‰è£…è‡ªå¸¦flashinfer,å‡ºç°çš„ä¸€äº›è«åé—®é¢˜
RuntimeError: pidfd_getfd: Operation not permittedï¼Œä½¿ç”¨PYTORCH_CUDA_ALLOC_CONF=expandable_segments:TrueåæŠ¥é”™ç­‰
æ›´æ–°ä»£ç é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh]

[2025-08-01 è§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜]

[2025-07-25 å¼€å¯NUMAå†…å­˜å ç”¨ä¸ç¿»å€]

æœ¬åˆ†æ”¯æä¾› NUMA ä¼˜åŒ–çš„ç¨³å®šç‰ˆæœ¬ï¼ŒåŒ…å«å†…å­˜/æ˜¾å­˜ä¼˜åŒ–å’Œä¸€äº›é—®é¢˜ä¿®å¤ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **NUMA å†…å­˜ä¼˜åŒ–**ï¼šå¤šä¸ª NUMA èŠ‚ç‚¹å…±äº«å•ä»½å†…å­˜ï¼Œ32èŠ‚ç‚¹è´Ÿè½½å‡è¡¡ï¼Œè§£ç é€Ÿåº¦ä¸é™ç•¥æœ‰æå‡
- **çº¿ç¨‹ç²¾ç»†æ§åˆ¶**ï¼šé€šè¿‡ `LK_THREADS` ç¯å¢ƒå˜é‡ç®¡ç†è®¡ç®—çº¿ç¨‹æ•°
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šæ–°å¢ `VLinearMarlin16` æ”¯æŒï¼Œè§£å†³ 16G æ˜¾å¡åŠ è½½ Kimi K2 çš„æ˜¾å­˜å³°å€¼é—®é¢˜
- **å·²éªŒè¯æ¨¡å‹**ï¼š
  - `KVCache-ai/Kimi-K2-Instruct-GGUF`
  - `deepseek-ai/DeepSeek-R1-0528`
  - `unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF`

## âš ï¸ å·²çŸ¥é—®é¢˜

1. æ”¯æŒ AMX çš„ CPU ä½¿ç”¨ amx é…ç½®æ–‡ä»¶ä¼šæŠ¥é”™ï¼ˆAMX åå°NUMAæ”¹é€ æœªå®Œæˆï¼‰
2. Prefill æ€§èƒ½ä¸‹é™ï¼ˆå·²è§£å†³ï¼‰
3. æŠ¥é”™ `RuntimeError: pidfd_getfd: Operation not permitted`å¯ä»¥å»æ‰PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True(å·²è§£å†³ï¼Œ é‡æ–°è¿è¡ŒUSE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh)
4. Intel è‡³å¼ºå¹³å°ï¼Œæˆ–è€…å¼€å¯è¶…çº¿ç¨‹è¿è¡Œé€Ÿåº¦æ…¢ï¼ˆå·²è§£å†³ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…æ­¥éª¤

git clone https://github.com/guqiong96/lktransformers.git

git checkout full-support-numa

git submodule update --init --recursive

USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh

### æ›´æ–°æºç 

git pull

USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh

### è¿è¡Œç¤ºä¾‹
LK_THREADS=62 python ~/Downloads/KTransformers/ktransformers/server/main.py \
    --gguf_path ~/Models/Kimi-K2-Instruct-GGUF  \
    --model_path ~/Models/Kimi-K2-Instruct-GGUF \
    --model_name Kimi-K2-Instruct-GGUF  \
    --cpu_infer 28 \
    --max_new_tokens 16384 \
    --cache_lens 16384 \
    --cache_q4 true \
    --temperature 0.6 \
    --top_p 0.95 \
    --optimize_config_path ~/Downloads/KTransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml \
    --force_think \
    --use_cuda_graph \
    --host 0.0.0.0 \
    --port 8070 \
    --max_batch_size 4 \
    --backend_type balance_serve \
    --chunk_size 1024

## ğŸ”§ é…ç½®æŠ€å·§

- **NUMA çº¿ç¨‹é…ç½®**ï¼š
  - å¼€å¯æœ€å¤šNUMAèŠ‚ç‚¹ï¼ŒAMD EPYC å¼€å¯ L3 As NUMA Domain 

- **æ˜¾å­˜ä¼˜åŒ–**ï¼šåœ¨ YAML é…ç½®ä¸­ä½¿ç”¨ `VLinearMarlin16` é˜²æ­¢ 16G æ˜¾å¡æ˜¾å­˜æº¢å‡º

## ğŸ“Œ æ³¨æ„äº‹é¡¹

1. æ›´æ–°åå‡ºç°ç–‘éš¾é—®é¢˜ï¼Œè¿è¡Œ USE_BALANCE_SERVE=1 USE_NUMA=1 bash install.sh
2. æ›´å¤šå®‰è£…é—®é¢˜è¯·å‚è€ƒä¸»çº¿æ–‡æ¡£
3. å®šæœŸåˆå¹¶ä¸»çº¿è·å–æœ€æ–°ç‰¹æ€§

![Weixin Image_20250818225938_10_147](https://github.com/user-attachments/assets/4ff43a0b-3055-44ea-9f96-558eb69f5486)


 
